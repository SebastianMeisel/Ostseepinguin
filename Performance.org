* USE Method
:PROPERTIES:
:header-args: :session *R* :cache yes :results value verbatim :exports both :tangle yes
:END:

To troubleshoot performance issues on a system it is important to use a systematic approach.
The USE Method by Greeg Brendan is such and approach. 

Look at [[https://brendangregg.com/usemethod.html]] (http only) for details. 

For that we want to evaluate

1 Utilization
2 Saturation
3 Errors

for all system elements.  

In this file I go through the main system resources and show tools check on this three topics. I loosly follow Gregg Brendan checklist: [[https://brendangregg.com/USEmethod/use-linux.html]].


Before I start, I have to set my locals to English (instead of German, that I normally use) for the commands in this file.

#+BEGIN_SRC bash 
export LC_ALL=en_US.utf8
S_COLORS=never
#+END_SRC

** CPU
*** Utilization 
***** vmstat
~vmstat~, when called without further option shows information on the CPU usage.
We are interested in the 13th, 14th and 17th column:
- US: user time - time CPU time consumed by user processes.
- SY: system time - time CPU time consumed by system processes.
- st: stolen time  - time time the virtual machine process are waiting on the physical CPU for their CPU time.

#+BEGIN_SRC bash 
  vmstat 1 3 | awk '{ print  $13 " " $14 " " $17}' | column -tc3
#+END_SRC

#+RESULTS[ab1616d5835201b12bfd5c13d6393a40d7a96587]:
| US | SY | st |
| 13 |  5 |  0 |
|  2 |  2 |  0 |
|  3 |  2 |  0 |

In the snippet I used awk to only print the columns I need, separated be a space. The column command is used for a better layout:
-t: fill columns.
-c3: three columns.

**** sar
For permanent monitor ~sasd~ from the ~sysstat~ package can be used. ~sar~ is used, to read the collected data.
Without the option ~-u~ (or without, as it is the default) you get information on the CPU. We are interested in all columns but ~%idle~ and ~%iowait~.
You can choose a time period of time with ~-s~ (start) and ~-e~(end)

#+BEGIN_SRC bash
sar -u -s 10:00 -e 11:00 | awk 'NR > 1 {print $1 " " $2 " " $3 " " $4 " " $6}' 
#+END_SRC

#+RESULTS[991573eab07ce16838ac69463fae5fd3d3570ca5]:
|               |     |       |       |         |
|      10:05:01 | CPU | %user | %nice | %iowait |
|      10:15:01 | all | 2,50  | 0,00  | 0,07    |
|      10:25:01 | all | 3,52  | 0,01  | 0,08    |
|      10:35:01 | all | 4,15  | 0,01  | 0,08    |
|      10:45:01 | all | 4,07  | 0,03  | 0,08    |
|      10:55:01 | all | 4,00  | 0,01  | 0,08    |
| Durchschnitt: | all | 3,65  | 0,01  | 0,08    |

Use ~-P ALL~ for multiple CPUs. You can replace ~ALL~ by a list of CPUs:

#+BEGIN_SRC bash
sar -P 1,3 -s 10:00 -e 10:20 | awk 'NR > 1 {print $1 " " $2 " " $3 " " $4 " " $6}' 
#+END_SRC

#+RESULTS:
|               |     |       |       |         |
|      10:05:01 | CPU | %user | %nice | %iowait |
|      10:15:01 |   1 | 2,42  | 0,00  | 0,05    |
|      10:15:01 |   3 | 2,34  | 0,00  | 0,04    |
| Durchschnitt: |   1 | 2,42  | 0,00  | 0,05    |
| Durchschnitt: |   3 | 2,34  | 0,00  | 0,04    |

In the I printed only the date (4th column) from the first line of the output: ~NR == 1 {print $4}~.
For the following lines I printed all but the 5th and 7th column: ~NR > 1 { print ...}~.

**** ps (top | htop)

You can also use ~top~ or ~htop~ to show the CPU usage. With ~ps~ we can just show this column - or add others of interest with the ~-o~ option:

#+BEGIN_SRC bash
ps -o pid,pcpu | sed -n '2,$p' | sort -rhk2 | head -5
#+END_SRC

#+RESULTS:
|  48013 | 2.6 |
|  48803 | 0.2 |
|  48500 | 0.2 |
| 239089 | 0.2 |
|  48562 | 0.1 |

For the sake of the snippet I concentrated on the first 5 lines. The ~sed~ command removes the header for sorting. This could also be done by adding the ~--no-header~option to the ~ps~command.
I sort the output with ~sort~:
-r: reverse order (highest values first).
-h: human number 1010 > 101 > 10.
-k2: second column is the key for sorting.

*** Saturation
**** vmstat, sar, dstat
For system-wide  monitoring of CPU saturation any of this tool can be used. ~sar~ could evaluate again data collected be ~sasd~. But as the other tools you can also use it for live monitoring.
Each command collected each ~3~ seconds data ~3~ times. 
We are interested in the number of processes in the run queue. This value should not be greater as as the number of CPU cores available. You can get this number from ~lscpu~s output or from ~/proc/cpuinfo~.  

#+BEGIN_SRC bash
lscpu | grep -m1 'CPU(s)'
echo
echo "vmstat"
vmstat 3 3 | awk 'NR > 1 {print $1}'
echo
echo "sar"
sar -q  3 3 | awk 'NR > 2 {print $2}'
echo
echo "dstat"
dstat -p 3 3 | awk 'NR > 1 {print $1}'
#+END_SRC

#+RESULTS:
| CPU(s): | 8 |
|         |   |
|  vmstat |   |
|       r |   |
|       2 |   |
|       0 |   |
|       1 |   |
|         |   |
|     sar |   |
| runq-sz |   |
|       2 |   |
|       2 |   |
|       2 |   |
|       2 |   |
|         |   |
|   dstat |   |
|     run |   |
|       0 |   |
|       0 |   |
|       0 |   |

**** /proc/[PID]/schedstat
The data in the file  ~schedstat~ in ~/proc/[PID]~ can be used to check the CPU saturation by single processes.
It contains three numbers:
1 CPU-time 
2 time in run queue.
3 number of time slices run on the CPU.

We are interested in the run queue time:

#+BEGIN_SRC bash
for d in $(find /proc/  -maxdepth 1 -type d -regex '.*[0-9]$') ; \
  do echo -n "$d: " ; \
  cut -f2 -d' ' $d/schedstat 2>/dev/null; \
done | sort -rhk2 | head -5
#+END_SRC

#+RESULTS:
| /proc/49685: | 1209460913065 |
| /proc/49752: | 1115469765976 |
| /proc/49784: |  993393130709 |
| /proc/51353: |  936296776909 |
| /proc/49686: |  824275814797 |

The ~for~ loop in the snippet go through all process directories.

For better relation you can also divide the value by the CPU-time:

#+BEGIN_SRC bash
for d in $(find /proc/  -maxdepth 1 -type d -regex '.*[0-9]$') ; \
    do echo -n "$d: " ; \
    echo $(awk '{print $2 " / " $1}' $d/schedstat 2>/dev/null | bc 2>/dev/null) ; \
done  | egrep -v '0$' | sort -rhk2 | head -5
#+END_SRC

#+RESULTS:
| /proc/567: | 1464 |
| /proc/13:  |   68 |
| /proc/54:  |   55 |
| /proc/32:  |   46 |
| /proc/20:  |   43 |

See how different the top five is.  

*** Errors
I don't know of a way to check for CPU errors on CLI.    

** Memory
*** Utilization
**** free
The first utility that comes to mind, when evaluating memory usage is of cause ~free~.
The ~-m~ uses mebibytes as the unit. 

#+BEGIN_SRC bash
   free -m
#+END_SRC

#+RESULTS:
| total | used | free | shared | buff/cache | available |      |
| Mem:  | 7839 | 5868 |    172 |        277 |      1798 | 1392 |
| Swap: | 8191 | 5454 |   2737 |            |           |      |

The ~~ just replaces the German locales on my system. 

**** vmstat
Again we can use ~vmstat~ for cumulative values. The option ~-SM~ again uses mebibytes.

#+BEGIN_SRC bash
 vmstat -SM | awk 'NR >  1 { print $3 " " $4 }'
#+END_SRC

#+RESULTS:
| swpd | free |
| 5459 |  167 |

**** sar
This is also a standard in the toolbox: ~sar -r~ (~-r~ for memory stats). The ~-h~option stands for 'human readable'.
We are interested in the column ~%memused~.

#+BEGIN_SRC bash
 sar -rh -s 15:00 -e 15:30 |\
 awk 'NR == 1 { print $4 } \
      NR > 2 {print $1 " " $5}'
#+END_SRC

#+RESULTS[d2c680fc368af5f7a0f2c5f126b7219665a151e0]:
:  sar -rh -s 15:00 -e 15:30 |\
: awk 'NR == 1 { print $4 } \
: NR > 2 {print $1 " " $5}'
: echo 'org_babel_sh_eoe'
: 03/06/21
: memused

**** dstat
You can also install ~dstat~, that gives you memory information with "-m" option.
 
#+BEGIN_SRC bash
 dstat -m  1 3 | awk ' NR >= 2 { print $2 }'
#+END_SRC

#+RESULTS:
| free |
| 169M |
| 161M |
| 182M |

**** slabtop
This tool requires root permission and provides a special inside. It show ressources of the so called slabs - groups of processes,
for which the kernel allocates cache of different size. The ~-s c~ option sorts by the cache size. The ~-o~option means 'once'.

#+BEGIN_SRC bash  #:dir  "/sudo::/tmp" :cache yes 
  sudo slabtop -os c | head -5
#+END_SRC

#+RESULTS[57ed92fd55d247fb034a8474168c40911d652cd8]:
| Objekte aktiv / gesamt (% benutzt) : 2128003 / 2377358 (89 |          5%) |          |     |
| Slabs aktiv / gesamt (% benutzt)   : 66978 / 66978 (100    |          0%) |          |     |
| Caches aktiv / gesamt (% benutzt)  : 119 / 160 (74         |          4%) |          |     |
| Größe aktiv / gesamt (% benutzt) : 442577                  | 98K / 503093 | 96K (88  | 0%) |
| Minimum / Durchschnitt / Maximum Objekt: 0                 |      01K / 0 | 21K / 14 | 75K |

  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
193565 158200  81%    1.07K  33053       29   1057696K ext4_inode_cache
568407 394356  69%    0.19K  27067       21    108268K dentry
 79408  57934  72%    0.57K   2836       28     45376K radix_tree_node

**** ps (top/htop)
You can also use ~top~ or ~htop~. But ~ps~ with the ~-o~ option can help us focus on the information we need. 
- pid: PID
- %mem: memory usage in %
- vsz:  virtual memory (swap)
- comm: command

#+BEGIN_SRC bash
ps -eo pid,%mem,vsz,comm | sort -rhk2,3 | column -tc4 | head -5
#+END_SRC

#+RESULTS:
| 239089 | 9.5 |  3246848 | qemu-system-x86 |
|  49752 | 5.3 |  5268108 | chrome          |
| 179473 | 4.6 | 13581076 | chrome          |
|  48500 | 4.2 |  4860304 | emacs           |
| 249614 | 3.7 |  4714372 | chrome          |

*** Saturation
**** vmstat
High swapping rates are a sign of memory saturation. In ~vmstat~ the columns ~si~ (swap in) and ~so~ (swap out) give us this information. 
#+BEGIN_SRC bash
 vmstat -SM | awk 'NR >  1 { print $7 " " $8 }'
#+END_SRC

#+RESULTS:
| si | so |
|  0 |  0 |

**** sar
 With the ~-B~ option ~sar~ reports pageing statistics. High values for pagescaning are in indicator for memory saturation.
 The corresponding columns are ~pgscank~for pages scanned by kswapd daemon and ~pgscand~ for directly scanned pages.

 #+BEGIN_SRC bash
  sar -B | awk 'NR == 3 {print $1 " " $7 " " $8 } ; END {print $1 " " $7 " " $8 }' | column -tc3
 #+END_SRC

 #+RESULTS:
 | 00:00:01 | pgscank/s | pgscand/s |
 | Average: |     31.58 |      0.68 |

 With the ~-W~ provides the swap in and swap out values in pages per second:

 #+BEGIN_SRC bash
  sar -W | sed -n '3p;$p' | column -tc3
 #+END_SRC

 #+RESULTS:
 | 00:00:01 | pswpin/s | pswpout/s |
 | Average: |     0.16 |      1.57 |


 **** /proc/[PID]/stat

 In this file the 10th value shows the minor fault rate, that can be an indicator for memory saturation on the process level - according to Gregg Brendan. 

 #+BEGIN_SRC bash
 cat /proc/33/stat | awk '{print $10}'
 #+END_SRC  

 #+RESULTS:
 : 0

**** dmesg | journalctl

I use earlyoom to prevent out of memory situations. I you do you check the output of ~journalctl~ with the ~-u~ option to choose messages from ~earlyoom.service~. 

#+BEGIN_SRC bash
 journalctl -u earlyoom.service --since 10:20 --until 10:25
#+END_SRC

#+RESULTS[bb60559972e1cd72a7a8dd3e151470eb99d0a6a8]:
#+begin_example
bash: bind: Warnung: Zeileneditierung ist nicht aktiviert.
bash: bind: Warnung: Zeileneditierung ist nicht aktiviert.
bash: bind: Warnung: Zeileneditierung ist nicht aktiviert.
bash: bind: Warnung: Zeileneditierung ist nicht aktiviert.
bash: bind: Warnung: Zeileneditierung ist nicht aktiviert.
unsetopt: Befehl nicht gefunden.
unsetopt: Befehl nicht gefunden.
unsetopt: Befehl nicht gefunden.
whence: Befehl nicht gefunden.
whence: Befehl nicht gefunden.
-- Logs begin at Wed 2020-10-28 22:04:43 CET, end at Sat 2021-03-06 20:41:11 CET. --
), swap free: 8187 of 8191 MiB (99 %)
), swap free: 8187 of 8191 MiB (99 %)
), swap free: 8187 of 8191 MiB (99 %)
), swap free: 8187 of 8191 MiB (99 %)
), swap free: 8187 of 8191 MiB (99 %)
#+end_example

In dmesg you can grep for processes that were killed by OOM killer:

#+BEGIN_SRC bash
dmesg | grep kill | tail -5
#+END_SRC

#+RESULTS:
| [277249.210926] | OOM | killer | enabled.  |
| [278365.770414] | OOM | killer | disabled. |
| [278368.957855] | OOM | killer | enabled.  |
| [283951.507185] | OOM | killer | disabled. |
| [283954.647615] | OOM | killer | enabled.  |

*** Error
You may install and use ~memtester~ for testing the memory, or look in the journal for physical failures.

** Network 
*** Utilization
**** ip
 While ~ifconfig~ is long deprecate, today ~ip~ is what comes first to mind for monitoring network usage.
 With the ~-s~ flag you get the statistics you need.
 #+BEGIN_SRC bash
 ip -s link show wlp1s0 | awk ' BEGIN { i=999 }
                       /^[1-9]/ {print $2} ; \
                       /RX/     {printf "%s", "RX: " ; i=(NR + 1)} ; \
                       /TX/     {printf "%s", "TX: " ; i=(NR + 1)} ; \
		       NR == i  {printf "%2.2f GiB\n", ($1/1024/1024/1024)}' 
 #+END_SRC

 #+RESULTS:
 | wlp1s0: |       |     |
 | RX:     | 11.78 | GiB |
 | TX:     |  1.49 | GiB |

 In the snippet, the ~awk~ code searches for lines start with a number, which indicates the beginning of a new section for a device, the name of which (second column) is then printed.
 Then it searches for the string 'RX' and 'TX' where a variable i is set to the line number increased by one, for which the first value  column is printed after converting bytes to gibibytes.

**** ifstat
 While ~ifconfig~ only gives you one sum up value, there are a lot of tools to watch the netload live. 
 One of them is ~ifstat~:
 -z Hide the interfaces which counters are null.
 -b Use kbits/sec instead of the default kbytes/sec.
 -T Show totals.

 #+BEGIN_SRC bash
 ifstat -zbT 1 3 |\
     awk 'NR == 1 {print "-" $1 "- -------- -" $2 "- -------- " } ; NR > 1 {print $0}' |\
     sed 's/s /s_/g'
 #+END_SRC

 #+RESULTS:
 | -wlp1s0- | -------- | -Total- | -------- |
 |  Kbps_in | Kbps_out | Kbps_in | Kbps_out |
 |     6.02 |    11.06 |    6.02 |    11.06 |
 |     0.67 |     1.78 |    0.67 |     1.78 |
 |    12.42 |     8.21 |   12.42 |     8.21 |

 The ~awl~ and ~sed~ statements in the snippet are just there to beautify the org-mode results.

**** sar

 #+BEGIN_SRC bash
  sar -n DEV 1 1 | awk 'NR > 2 && $3 != 0 {print $1 " " $2 " " $3 " " $4 " " $5 " " $6}'
 #+END_SRC

 #+RESULTS:
 | 21:12:37 | IFACE  | rxpck/s | txpck/s | rxkB/s | txkB/s |
 | 21:12:38 | wlp1s0 |     4.0 |     1.0 |   0.28 |   0.57 |
 |          |        |         |         |        |        |
 | Average: | IFACE  | rxpck/s | txpck/s | rxkB/s | txkB/s |
 | Average: | wlp1s0 |     4.0 |     1.0 |   0.28 |   0.57 |


**** dtstat

 #+BEGIN_SRC bash
  dstat -n 1 3
 #+END_SRC

 #+RESULTS:
 | -net/total- |       |
 | recv        |  send |
 | 0           |     0 |
 | 1349B       | 1041B |
 | 81B         |     0 |

**** nicstat

 #+BEGIN_SRC bash
 nicstat -zM | awk '{$9=$10=""; print $0}'
 #+END_SRC

 #+RESULTS:
 |     Time | Int    | rMbps | wMbps | rPk/s | wPk/s |   rAvs |  wAvs |
 | 18:26:23 | lo     |   0.0 |   0.0 |  0.37 |  0.37 |  122.1 | 122.1 |
 | 18:26:23 | wlp1s0 |  0.12 |  0.02 | 15.15 |  5.94 | 1076.5 | 356.3 |



**** collected

 #+BEGIN_SRC bash
  collectl -sn -oT -i05 -c3 | awk 'NR > 2 {print $0}'
 #+END_SRC

 #+RESULTS:
 |    #Time | KBIn | PktIn | KBOut | PktOut |
 | 23:00:39 |    0 |     0 |     0 |      0 |
 | 23:00:44 |    0 |     0 |     0 |      0 |
 | 23:00:49 |    0 |     0 |     0 |      0 |

**** /proc/net/dev

 #+BEGIN_SRC bash
 cat /proc/net/dev |\
 awk 'NR == 1 {print $1 $2" -------- "$4" --------"} ; \
      NR == 2 {print $1 " MBytes packets MBytes packets"}
      NR >  2 {print $1 " " ($2 / 1000000) " " $3 " " ($10 / 1000000) " " $11}'
 #+END_SRC

 #+RESULTS:
 | Inter-   | Receive | -------- | Transmit | -------- |
 | face     |  MBytes |  packets |   MBytes |  packets |
 | lo:      | 33.4465 |   270022 |  33.4465 |   270022 |
 | ens1f1:  |       0 |        0 |        0 |        0 |
 | wlp1s0:  | 12780.9 | 11663216 |  1617.72 |  4494804 |
 | docker0: |       0 |        0 |        0 |        0 |

*** Saturation
**** nicstat
 This is the only tool that provides you with a dedicated column for network saturation, which is calculated from different kernel statistic as errors/second:

 #+BEGIN_SRC bash
 nicstat | awk '{print $1 " " $10}'
 #+END_SRC

 #+RESULTS:
 |     Time | Sat |
 | 21:46:23 | 0.0 |
 | 21:46:23 | 0.0 |

 With the ~-x~ option you get more detailed statistics:

 #+BEGIN_SRC bash
 nicstat -x 
 #+END_SRC

 #+RESULTS:
 | 21:45:33 |  RdKB |  WrKB | RdPkt | WrPkt | IErr | OErr | Coll | NoCP | Defer | %Util |
 | lo       |  0.17 |  0.17 |  1.91 |  1.91 |  0.0 |  0.0 |  0.0 |  0.0 |   0.0 |   0.0 |
 | wlp1s0   | 53.82 | 683.3 | 435.7 | 402.4 |  0.0 |  0.0 |  0.0 |  0.0 |   0.0 |   0.0 |


**** ip
 A clear sign of network saturation are overruns packets. Also a high number of dropped packets may give you a clue.
 Both can be shown with the ~ip~ command with ~-s~ option and ~link~ parameter: 

 #+BEGIN_SRC bash
 ip -s link show wlp1s0 | awk ' BEGIN { i=999 ; j=999}
                       /^[1-9]/ {print $2} ; \
                       /RX/     {printf "%s", "RX: " ; i=(NR + 1)} ; \
                       /TX/     {printf "%s", "TX: " ; j=(NR + 1)} ; \
		       NR == i  {printf "Dropped: %d Overrun: %d \n", $4, $5 } ; \
		       NR == j  {printf "Dropped: %d", $4 } ' 
 #+END_SRC

 #+RESULTS:
 | wlp1s0: |          |       |          |   |
 | RX:     | Dropped: | 58090 | Overrun: | 0 |
 | TX:     | Dropped: |     0 |          |   |

**** sar
 This statistics can also be collected by ~sasd~ and show by ~sar -n EDEV~:

 #+BEGIN_SRC bash
  sar -n EDEV -s 19:30 -e 20:00 | awk 'NR > 2 {print $6 " " $7 " " $10 " " $11}'
 #+END_SRC

 #+RESULTS:
 | rxdrop/s | txdrop/s | rxfifo/s | txfifo/s |
 |      0.0 |      0.0 |      0.0 |      0.0 |
 |      0.0 |      0.0 |      0.0 |      0.0 |
 |      0.0 |      0.0 |      0.0 |      0.0 |
 |      0.0 |      0.0 |      0.0 |      0.0 |
 |      0.0 |      0.0 |      0.0 |      0.0 |
 |      0.0 |      0.0 |      0.0 |      0.0 |
 |      0.0 |      0.0 |      0.0 |      0.0 |
 |      0.0 |      0.0 |      0.0 |      0.0 |

**** ss
 With ~ss~ you can get the number of dropped packets per socket with the ~-m~ (or ~--memory~) option:

 #+BEGIN_SRC bash
 ss -tuam | sed -E 's/skmem[:(].*?d([0-9]+)[)]/Dropped: \1/g;
                   s/\b0\b/--/g;
                   s/([0-9]{1,3})(.([0-9]{1,3})){3,3}/⌷⌷⌷.⌷⌷⌷.⌷⌷⌷.⌷⌷⌷/g' |\
 head
 #+END_SRC

 #+RESULTS:
 | Netid | State    | Recv-Q | Send-Q | Local                  | Address:Port        | Peer | Address:Port | Process |
 | udp   | ESTAB    | --     | --     | ⌷⌷⌷.⌷⌷⌷.⌷⌷⌷.⌷⌷⌷:46459  | ⌷⌷⌷.⌷⌷⌷.⌷⌷⌷.⌷⌷⌷:443 |      |              |         |
 |       | Dropped: | --     |        |                        |                     |      |              |         |
 | udp   | ESTAB    | --     | --     | ⌷⌷⌷.⌷⌷⌷.⌷⌷⌷.⌷⌷⌷:42751  | ⌷⌷⌷.⌷⌷⌷.⌷⌷⌷.⌷⌷⌷:443 |      |              |         |
 |       | Dropped: | --     |        |                        |                     |      |              |         |
 | udp   | ESTAB    | --     | --     | ⌷⌷⌷.⌷⌷⌷.⌷⌷⌷.⌷⌷⌷:43387  | ⌷⌷⌷.⌷⌷⌷.⌷⌷⌷.⌷⌷⌷:443 |      |              |         |
 |       | Dropped: | --     |        |                        |                     |      |              |         |
 | udp   | UNCONN   | --     | --     | --.--.--.--:48336      | --.--.--.--:*       |      |              |         |
 |       | Dropped: | --     |        |                        |                     |      |              |         |
 | udp   | UNCONN   | --     | --     | 127.--.--.53%lo:domain | --.--.--.--:*       |      |              |         |

 The ~sed~command in the snippet does (by line):
     1 Extract the dropped packets from the output of the ~-m~ option.
     2 Replace zero values by '--' for better overview.
     3 Hide the IP-addresses in the out for privacy reasons.

**** /proc/dev/net

 You can get the same results from the ~/proc/~ file system:

 #+BEGIN_SRC bash
 cat /proc/net/dev |\
 awk 'NR == 1 {print $1 $2 " -- " $4 " -- "};\
      NR == 2 {print $1 " RX-drops RX-overrun TX_drops RX-overrun" };\
      NR >  2 {print $1 " " $5 " " $6 " " $14 " " $15}'
 #+END_SRC

 #+RESULTS:
 | Inter-   |  Receive |         -- | Transmit |         -- |
 | face     | RX-drops | RX-overrun | TX_drops | RX-overrun |
 | lo:      |        0 |          0 |        0 |          0 |
 | ens1f1:  |        0 |          0 |        0 |          0 |
 | wlp1s0:  |        0 |          0 |        0 |          0 |
 | docker0: |        0 |          0 |        0 |          0 |

**** /sys/class/net/[device]/statistics 
                                        
 … and from the ~sys~ file system. 
 #+BEGIN_SRC bash 
 egrep '[0-9]+' /sys/class/net/*/statistics/* |\
 egrep '(drop|fifo)' |& \
 sed 's|:|/|g'|\
 awk -F '/' 'BEGIN     {DEV = ""};\
             DEV != $5 {print $5 "-------------- ---"} ; \
                       {print " *" $7 " " $8 ; DEV = $5}' ; 
 #+END_SRC 

 #+RESULTS:
 | docker0-------------- | --- |
 | *rx_dropped           |   0 |
 | *rx_fifo_errors       |   0 |
 | *tx_dropped           |   0 |
 | *tx_fifo_errors       |   0 |
 | ens1f1--------------  | --- |
 | *rx_dropped           |   0 |
 | *rx_fifo_errors       |   0 |
 | *tx_dropped           |   0 |
 | *tx_fifo_errors       |   0 |
 | lo--------------      | --- |
 | *rx_dropped           |   0 |
 | *rx_fifo_errors       |   0 |
 | *tx_dropped           |   0 |
 | *tx_fifo_errors       |   0 |
 | wlp1s0--------------  | --- |
 | *rx_dropped           |   0 |
 | *rx_fifo_errors       |   0 |
 | *tx_dropped           |   0 |
 | *tx_fifo_errors       |   0 |

 The data is provided in different files (one file per value). I use ~egrep~ here to extract the data here, because this way I also get the file name in the output.
 I then grep for file name that contain 'drop' (for dropped packets) and 'fifo' (for fifo errors).
 With the ~sed~ command I replace the colon,  ~egrep~ places between the file name and the value by a slash, that ~awk~ with the ~-F~ option then uses as a column delimeter for a nice and readable output.  

*** Error
**** ip
 The first look network errors should be ~ip~ with the ~-s~ option for statistics. Look out for RX- and TX-errors. 

 #+BEGIN_SRC bash
 ip -s link show wlp1s0 | awk 'BEGIN { i=999 }
                 /^[1-9]/ {print $2} ; \
                 /RX/    {printf "%s", "RX-errors: " ; i=(NR + 1)} ; \
                 /TX/    {printf "%s", "TX-errors: " ; i=(NR + 1)} ; \
		 NR == i {print $3}' 
 #+END_SRC

 #+RESULTS:
 | wlp1s0:    |   |
 | RX-errors: | 0 |
 | TX-errors: | 0 |

**** sar
 Again can you also use the swith army knive of monitoring: ~sar -n EDEV~ provides you with the disired information. 

 #+BEGIN_SRC bash
  sar -n EDEV 1 1 | awk 'NR > 2 {print $1 " " $2 " " $3 " " $4}'
 #+END_SRC

 #+RESULTS:
 | 19:41:49 | IFACE   | rxerr/s | txerr/s |
 | 19:41:50 | lo      |     0.0 |     0.0 |
 | 19:41:50 | ens1f1  |     0.0 |     0.0 |
 | 19:41:50 | wlp1s0  |     0.0 |     0.0 |
 | 19:41:50 | docker0 |     0.0 |     0.0 |
 |          |         |         |         |
 | Average: | IFACE   | rxerr/s | txerr/s |
 | Average: | lo      |     0.0 |     0.0 |
 | Average: | ens1f1  |     0.0 |     0.0 |
 | Average: | wlp1s0  |     0.0 |     0.0 |
 | Average: | docker0 |     0.0 |     0.0 |

**** nicstat
 You can get the information with the ~-x~ option in the 'IErr' and 'OErr' colums.

 #+BEGIN_SRC bash
 nicstat -x | awk '{print $1 " " $6 " " $7}'
 #+END_SRC

 #+RESULTS:
 | 11:02:04 | IErr | OErr |
 | lo       |  0.0 |  0.0 |
 | wlp1s0   |  0.0 |  0.0 |

**** /proc/dev/net
 And again the ~/proc/~ filesystem is your friend …

 #+BEGIN_SRC bash
 cat /proc/net/dev |\
 awk 'NR == 1 {print $1 $2 " " $4 };\
      NR == 2 {print $1 " RX_Errors TX_Errors" };\
      NR >  2 {print $1 " " $4 " " $13}'
 #+END_SRC

 #+RESULTS:
 | Inter-   |   Receive |  Transmit |
 | face     | RX_Errors | TX_Errors |
 | lo:      |         0 |         0 |
 | ens1f1:  |         0 |         0 |
 | wlp1s0:  |         0 |         0 |
 | docker0: |         0 |         0 |

**** /sys/class/net/[device]/statistics 
… as is the ~sys~ filesystem.
                                        
#+BEGIN_SRC bash 
egrep '[0-9]+' /sys/class/net/*/statistics/*_errors |\
sed 's/:/\//' |\
awk -F '/' 'BEGIN     {DEV = ""};\
            DEV != $5 {print $5 "-------------- ---"} ; \
                      {print " *" $7 " " $8 ; DEV = $5}' |\
head -14
#+END_SRC 

#+RESULTS:
| docker0-------------- | --- |
| *rx_crc_errors        |   0 |
| *rx_errors            |   0 |
| *rx_fifo_errors       |   0 |
| *rx_frame_errors      |   0 |
| *rx_length_errors     |   0 |
| *rx_missed_errors     |   0 |
| *rx_over_errors       |   0 |
| *tx_aborted_errors    |   0 |
| *tx_carrier_errors    |   0 |
| *tx_errors            |   0 |
| *tx_fifo_errors       |   0 |
| *tx_heartbeat_errors  |   0 |
| *tx_window_errors     |   0 |
** Storage
*** I/O
**** Utilization  
***** iostat 
 When you call ~iostat~ with the ~-x~ option for extended statistics you find the utilization percentage in the last column.

 #+BEGIN_SRC bash
  iostat -xz 1 1 | awk 'NR == 6 {print $1 " " $NF}
                                 NR > 6 && $(NF) != 0 && $0 != "" {print  $1 " " $NF }' 
 #+END_SRC

 #+RESULTS:
 | Device | %util |
 | sda    |  0.22 |

***** sar

And as almost ever you can get the same information from ~sar~ with the ~-d~ option and also in the last column.

#+BEGIN_SRC bash
 sar -d -s 15:00 -e 15:30 |\
awk 'NR == 3            {print $1 " " $2 " " $NF};
     NR > 3 && $NF != 0 {print $1 " " $2 " " $NF}'
#+END_SRC

#+RESULTS:
| 15:15:01 | DEV    | %util |
| 15:25:01 | dev8-0 |  0.87 |
| 15:25:01 | dev8-2 |  0.86 |
| Average: | dev8-0 |  0.87 |
| Average: | dev8-2 |  0.86 |

***** iotop 

  On a process level ~iotop~ gives you the information you need. The tool is generally interactive, but can be run in batch mode (~-b~) with a specified  iteration (~-n~) and delay (~-d~):

  #+BEGIN_SRC bash 
  sudo iotop -ob -n 2 -d 1
  #+END_SRC

  #+RESULTS:
  | Total   | DISK | READ:    |  0.0 | B/s  |       |       | Total   | DISK | WRITE:  |   0.0 | B/s                                 |       |
  | Current | DISK | READ:    |  0.0 | B/s  |       |       | Current | DISK | WRITE:  |   0.0 | B/s                                 |       |
  | TID     | PRIO | USER     | DISK | READ |  DISK | WRITE | SWAPIN  | IO   | COMMAND |       |                                     |       |
  | Total   | DISK | READ:    |  0.0 | B/s  |       |       | Total   | DISK | WRITE:  | 76.22 | K/s                                 |       |
  | Current | DISK | READ:    |  0.0 | B/s  |       |       | Current | DISK | WRITE:  |  5.29 | M/s                                 |       |
  | TID     | PRIO | USER     | DISK | READ |  DISK | WRITE | SWAPIN  | IO   | COMMAND |       |                                     |       |
  | 40873   | be/4 | root     |  0.0 | B/s  |   0.0 | B/s   | 0.0     | %    | 2.06    |     % | [kworker/u16:0-ext4-rsv-conversion] |       |
  | 345     | be/3 | root     |  0.0 | B/s  | 64.79 | K/s   | 0.0     | %    | 0.63    |     % | [jbd2/sda2-8]                       |       |
  | 34083   | be/4 | sebastia |  0.0 | B/s  | 11.43 | K/s   | 0.0     | %    | 0.28    |     % | insync                              | start |
  
***** pidstat

  Annother tool is ~pidstat~ with ~-d~ option, that doesn't need root permissions:

  #+BEGIN_SRC bash
   pidstat -d | sed -n '3,9p'
  #+END_SRC

  #+RESULTS:
  | 21:39:30 | UID | PID | kB_rd/s | kB_wr/s | kB_ccwr/s | iodelay | Command         |
  | 21:39:30 |   0 |   1 |    -1.0 |    -1.0 |      -1.0 |     222 | systemd         |
  | 21:39:30 |   0 | 128 |    -1.0 |    -1.0 |      -1.0 |    1463 | kswapd0         |
  | 21:39:30 |   0 | 345 |    -1.0 |    -1.0 |      -1.0 |    6226 | jbd2/sda2-8     |
  | 21:39:30 |   0 | 404 |    -1.0 |    -1.0 |      -1.0 |     390 | systemd-journal |
  | 21:39:30 |   0 | 453 |    -1.0 |    -1.0 |      -1.0 |       2 | loop1           |
  | 21:39:30 |   0 | 462 |    -1.0 |    -1.0 |      -1.0 |       2 | loop2           |

**** Saturation 
***** iostat
To check for I/O saturation we need the extended statistics (~-x~). 
We are only interested in device that produce I/O in the sample time (~-z~).
I highly recommend the ~-h~ option for a human readable output.

We are interested in the await times the should go to high. But our main interest should be the average queue length of I/O request (~aqu-sz~), which should never got above 1.

#+BEGIN_SRC bash
 \
iostat -hxz 1 1 |\
awk 'NR > 5 && $5 != 0 && NF > 3 {print $5 " " $NF};
     NF == 3 && $1 != 0           {print $1 " " $NF}'|\
grep -v loop
#+END_SRC

#+RESULTS:
| r_await | Device |
|    1.52 | sda    |
| w_await | Device |
|    0.32 | sda    |
| d_await | Device |
|    1.08 | sda    |
|  aqu-sz | Device |
|    0.01 | sda    |

The output of ~iostat~ contains 5 data set. In the snippet I skip the first, about the average CPU usage (~NR > 5~).
The I check the 2 data sets on read, write and discard request, which are all more the 3 column (~NF > 3~) and check if the await values in the 5th column are greater the 0 (~$5 > 0~), before printing that value and the device name in the last column (~$NF).
Finally I print the 1st and last column (~aqu-sz~ and ~Device~) from the last data set, that is 3 columns wide (~NF == 3~). 
I also don't care for loop devices here, so I filter the out (~grep -v~).

***** sar
And yes, you *can* get the same data with ~sar~:

#+BEGIN_SRC bash
 \
sar -hd -s 11:00 -e 11:20 |\
awk '($7 > 0 || $8 >0)  && NR > 2 {print $1 " " $7 " " $8 " " $NF}'
#+END_SRC

#+RESULTS:
| 11:05:01 | aqu-sz | await | DEV  |
| 11:15:01 |   0.03 |  0.22 | sda  |
| 11:15:01 |   0.02 |  0.22 | sda2 |
| 11:15:01 |    0.0 |   4.0 | sda3 |
| Average: |   0.03 |  0.22 | sda  |
| Average: |   0.02 |  0.22 | sda2 |
| Average: |    0.0 |   4.0 | sda3 |

**** Errors
***** smartctl 
The dedicated tools for finding storage I/O device errors is ~smartctl~. It's not preinstalled on most distributions.
You can call it with the ~-l errors~ option, and if any errors are reported you can take a deep dive with ~-a~ (all SMART information about the disk) or ~-x~ to even include non-SMART information.
 
#+BEGIN_SRC bash :results value verbatim
sudo smartctl -l error /dev/sda
#+END_SRC

#+RESULTS[495e8a0f49313864cf0a9b294fadb9f134fbb229]:
: smartctl 7.1 2019-12-30 r5022 [x86_64-linux-5.8.0-44-generic] (local build)
: Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org
: 
: === START OF READ SMART DATA SECTION ===
: SMART Error Log Version: 1
: No Errors Logged

***** /sys/devices/.../ioerr_cnt  
If you don't have ~smartctl~ installed you still some information on I/O errors in the ~/sys/~ file system.
The error count is given as hexadecimal number.

#+BEGIN_SRC bash
find /sys/devices/ -iname 'ioerr_cnt' |\
 xargs cat | sed 's/0x//' | xargs echo 'ibase=16; ' | bc 
#+END_SRC

#+RESULTS[997f0381c6310cf78bcd027dd5accc1699b3ae60]:
| find  | /sys/devices/ | -iname | 'ioerr_cnt' |     | \        |   |   |       |      |            |   |   |   |    |
| xargs | cat           |        |             | sed | 's/0x//' |   |   | xargs | echo | 'ibase=16; | ' |   |   | bc |
| 352   |               |        |             |     |          |   |   |       |      |            |   |   |   |    |

*** Capacity
**** Utilization
***** df
To get an overview over the usage of your devices, ~df~ is the first choice. You can make it human readable (~-h~) and exclude all those virtual file system of modern systems for better overview (~-x~ ...).  

 #+BEGIN_SRC bash
  \
 df -h -x tmpfs -x devtmpfs -x squashfs 
 #+END_SRC

 #+RESULTS[b15908347aafad08dd8e1017ed62763df20b7cad]:
 |  | \  |    |       |    |          |    |          |
 | df        | -h | -x | tmpfs | -x | devtmpfs | -x | squashfs |
 | Mounted   | on |    |       |    |          |    |          |

***** du 
You can use ~du~ if a storage drive is overfull for further investigation. 

#+BEGIN_SRC bash
 \
du -d 1 -h | sort -h | sed -En '/[0-9]G/p' |\
sed -E 's|/[.a-zA-Z@]+|********|'
#+END_SRC

#+RESULTS[c6a2410f7cb29f7ab5399c588116b5535b6e7b06]:
|  | \         |    |              |          |   |      |    |   |   |     |     |             |   |   |
| du        | -d        |  1 | -h           |          |   | sort | -h |   |   | sed | -En | '/[0-9]G/p' |   | \ |
| sed       | -E        | 's | /[.a-zA-Z@]+ | ******** | ' |      |    |   |   |     |     |             |   |   |
| 1.2G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 1.5G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 1.6G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 1.7G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 1.9G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 2.4G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 2.5G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 2.9G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 30G       | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 3.3G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 4.4G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 5.7G      | .******** |    |              |          |   |      |    |   |   |     |     |             |   |   |
| 61G       | 0         |    |              |          |   |      |    |   |   |     |     |             |   |   |
     
**** Saturation 
Actually you will be told by linux when you have run out of disk space and if you really do you can not actually do a lot any more. 
Most of the times have do go to rescue mode and first delete some data you are sure, you don't need any more, before you can use do any research on the reasons by examining the storage utilisation. 
**** Errors
I don't actually know what errors you could look here for. 
